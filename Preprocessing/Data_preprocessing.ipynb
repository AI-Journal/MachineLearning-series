{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NumPy**\n",
        "\n",
        "NumPy is a Powerful Python library for numerical operations and array manipulation.\n",
        "\n",
        "**Matplotlib**\n",
        "\n",
        "Matplotlib.pyplot is a module in Matplotlib library used for creating visualizations in python.\n",
        "\n",
        "**Pandas**\n",
        "\n",
        "Pandas is a versatile Python library for data manipulation and analysis, especially with tabular data structures like DataFrames.\n",
        "\n"
      ],
      "metadata": {
        "id": "RS7HWVxB34po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "E1hFcMnzFp4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide the dataset into independent and dependent variables.\n",
        "\n",
        "Independent Variables - Features or input variables.\n",
        "\n",
        "Dependent variables - Target variables"
      ],
      "metadata": {
        "id": "Qga_06374qr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/Data.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "06GzyIArlae9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0D6Ypojl13P",
        "outputId": "1db33297-23a9-4d0e-cd3c-ebfce3a32dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqWZs2b-l4Rf",
        "outputId": "a5b67ed3-f435-4f00-b81c-9b33085f6811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason to handle Missing Data?**\n",
        "\n",
        "Generally the missing values will prone to make errors when training machine learning model.\n",
        "\n",
        "Ways to handle:\n",
        "1. We can either ignore the entire observation - This works when there is a larger dataset and there are only few missing values\n",
        "\n",
        "2. Replace missing value by average of all the values in the column\n",
        "\n",
        "As we have smaller dataset , we will be doing the second way to handle missing data.\n"
      ],
      "metadata": {
        "id": "lKtpG0dz5rO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
        "imputer.fit(X[:,1:3])\n",
        "X[:,1:3] = imputer.transform(X[:,1:3])\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYZuKWnupKsF",
        "outputId": "10d8c366-d0b1-409d-a0fa-805c0d8d80fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason why to do one hot encoding ?**\n",
        "\n",
        "We need to encode categorical variables in machine learning because most machine learning algorithms require numerical input data. Categorical variables, such as colors or categories, need to be converted into numerical form for the algorithms to process them effectively.\n",
        "\n",
        "**When to use LabelEncoder and OneHotEncoder?**\n",
        "\n",
        "When deciding between LabelEncoder and One-Hot Encoder, it's essential to consider the nature of the categorical variable.\n",
        "- Use LabelEncoder when the categorical variable has an inherent order or hierarchy, like low, medium, high. LabelEncoder assigns a unique numerical value to each category based on their order.\n",
        "- Use One-Hot Encoder when the categorical variable has no intrinsic order or hierarchy. One-Hot Encoder creates binary columns for each category, where each column represents a category with a value of 0 or 1, indicating the presence or absence of that category in the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "aT43SlgU6vkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "metadata": {
        "id": "AkNA6q6yxhvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C8RJW4CzPM2",
        "outputId": "bba4e885-74c1-4a55-d228-1d72d20c74b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
              "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
              "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
              "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
              "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
              "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
              "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
              "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
              "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
              "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "xKhwe-MxhfEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGj4MjzphuU3",
        "outputId": "dd6519dd-2eaf-4b5d-cd29-e8154c606a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Splitting the dataset -\n",
        "\n",
        "Train set - We train machine learning model on existing observations\n",
        "\n",
        "Test Set - Evaluate performance on new observations\n",
        "\n"
      ],
      "metadata": {
        "id": "yZN9fHEBBZih"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTkNZ7oGosHD",
        "outputId": "e0ab2c8f-b4bf-479c-abee-b4c6edf21d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0, 0.0, 0.0, 35.0, 58000.0],\n",
              "       [1.0, 0.0, 0.0, 44.0, 72000.0],\n",
              "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
              "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
              "       [1.0, 0.0, 0.0, 37.0, 67000.0],\n",
              "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
              "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
              "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piaNJ-LCpJra",
        "outputId": "5bc76fc1-36e5-4365-bd5b-2b664c7bce6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRILQ_GLpLdw",
        "outputId": "702457d8-ddd3-4d19-8f06-5c8ab5a4d3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0, 1.0, 0.0, 50.0, 83000.0],\n",
              "       [0.0, 0.0, 1.0, 27.0, 48000.0]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134E3iTIpM9X",
        "outputId": "c2b0906f-a02b-4d30-b852-2d5d1eb8864e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FAQ:**\n",
        "\n",
        "**Do we have to apply Feature Scaling before Splitting the dataset or after and why?**\n",
        "\n",
        "We have to apply feature scaling after splitting the dataset.\n",
        "\n",
        "We perform feature scaling after the train-test split to prevent data leakage. If we scale the features before splitting the data into training and testing sets, information from the test set could influence the training set, leading to biased results. By scaling the features after the split, we ensure that the scaling is based only on the training data, maintaining the integrity of the test set for evaluation.\n",
        "\n",
        "**Why Feature Scaling? **\n",
        "\n",
        "Feature scaling is important in machine learning to ensure that all features have a similar scale or range. This is crucial because many machine learning algorithms perform better or converge faster when features are on a similar scale.\n",
        "\n",
        "Imagine you have one feature that ranges from 0 to 1000 and another feature that ranges from 0 to 1. The algorithm might give more importance to the feature with a larger range, leading to biased results. Feature scaling helps to level the playing field by bringing all features to a similar scale, making the algorithm more effective in analyzing the data accurately.\n",
        "\n",
        "**Types of Feature Scaling**\n",
        "\n",
        "The two main types of feature scaling are Min-Max scaling and Standardization (Z-score normalization).\n",
        "\n",
        "- Min-Max scaling (Normalization): This method scales the data to a fixed range, usually between 0 and 1. It is calculated as (X - X_min) / (X_max - X_min). Use Min-Max scaling when you need the data to be within a specific range and when the distribution of the data is not Gaussian.\n",
        "\n",
        "- Standardization (Z-score normalization): This method scales the data so that it has a mean of 0 and a standard deviation of 1. It is calculated as (X - mean) / standard deviation. Use Standardization when the data follows a Gaussian distribution, as it does not bound the values to a specific range but centers them around the mean.\n",
        "\n"
      ],
      "metadata": {
        "id": "ODd3XCc5CSGU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train[:,3:] = sc.fit_transform(x_train[:,3:])\n",
        "x_test[:,3:]=sc.transform(x_test[:,3:])\n"
      ],
      "metadata": {
        "id": "F_6W0kY4tcIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVLW1ZO4uEvw",
        "outputId": "b279efae-5217-4657-9c2a-39f5f5181bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.0, 0.0, 0.0, -0.7529426005471072, -0.6260377781240918],\n",
              "       [1.0, 0.0, 0.0, 1.008453807952985, 1.0130429500553495],\n",
              "       [1.0, 0.0, 0.0, 1.7912966561752484, 1.8325833141450703],\n",
              "       [0.0, 1.0, 0.0, -1.7314961608249362, -1.0943465576039322],\n",
              "       [1.0, 0.0, 0.0, -0.3615211764359756, 0.42765697570554906],\n",
              "       [0.0, 1.0, 0.0, 0.22561095973072184, 0.05040823668012247],\n",
              "       [0.0, 0.0, 1.0, -0.16581046438040975, -0.27480619351421154],\n",
              "       [0.0, 0.0, 1.0, -0.013591021670525094, -1.3285009473438525]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "id": "dm2MyIvbuS6U",
        "outputId": "d51cb550-9577-415a-e305-8b4fde8bba9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0, 1.0, 0.0, 2.1827180802863797, 2.3008920936249107],\n",
              "       [0.0, 0.0, 1.0, -2.3186282969916334, -1.7968097268236927]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### These are the links for reference, kindly visit these for more information\n",
        "- [Scikit-learn : Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
        "- [Numpy](https://numpy.org/doc/)\n",
        "- [Pandas](https://pandas.pydata.org/docs/)\n",
        "- [Matplotlib](https://matplotlib.org/stable/index.html)"
      ],
      "metadata": {
        "id": "9h5py7EeUiyP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-3iyuXquUXX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}